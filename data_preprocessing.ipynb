{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e82d30fe-e02f-48c0-97dd-df6da4b7ec12",
   "metadata": {
    "id": "u9hcW97IpFLP",
    "papermill": {
     "duration": 0.23434,
     "end_time": "2023-12-09T21:36:17.845916",
     "exception": false,
     "start_time": "2023-12-09T21:36:17.611576",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Constants and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63150de1-60dd-46fd-b6fa-a56c5d3078b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux\n",
      "\n",
      "PROJECT_FOLDER = ''\n",
      "SRC_FOLDER = ''\n",
      "OUTPUT_FOLDER = ''\n",
      "\n",
      "TUAB_DIRECTORY = 'Data/TUAB/'\n",
      "TUAB_TRAIN = 'Data/TUAB/train/normal/01_tcp_ar/'\n",
      "TUAB_EVAL = 'Data/TUAB/eval/normal/01_tcp_ar/'\n",
      "\n",
      "DEPR_ANON_DIRECTORY = 'Data/depression_anonymized/'\n",
      "\n",
      "INHOUSE_DIRECTORY = 'Data/inhouse_dataset/EEG_baseline_with_markers_cleaned/preprocessed_data/EEG_baseline/'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "In this section one defines environment variables. \n",
    "Because I used this notebook on number of machines, I implemented class especially for this. \n",
    "You may not needed in one and use just simple definitions.\n",
    "'''\n",
    "\n",
    "from system_variables import SystemVariables\n",
    "\n",
    "# choose system according your current machine\n",
    "# SYSTEM_NAME = \"Windows\"\n",
    "# SYSTEM_NAME = \"Colab\"\n",
    "# SYSTEM_NAME = \"Kaggle\"\n",
    "SYSTEM_NAME = \"Linux\"\n",
    "\n",
    "sv = SystemVariables(SYSTEM_NAME)\n",
    "PROJECT_FOLDER = sv.get_project_folder()\n",
    "SRC_FOLDER = sv.get_src_folder()\n",
    "OUTPUT_FOLDER = sv.get_output_folder()\n",
    "TUAB_DIRECTORY, TUAB_TRAIN, TUAB_EVAL = sv.get_TUAB_folders()\n",
    "DEPR_ANON_DIRECTORY = sv.get_depr_anon_folder()\n",
    "INHOUSE_DIRECTORY = sv.get_inhouse_folder()\n",
    "\n",
    "print(SYSTEM_NAME)\n",
    "print()\n",
    "\n",
    "print(f\"{PROJECT_FOLDER = }\")\n",
    "print(f\"{SRC_FOLDER = }\")\n",
    "print(f\"{OUTPUT_FOLDER = }\")\n",
    "print()\n",
    "\n",
    "print(f\"{TUAB_DIRECTORY = }\")\n",
    "print(f\"{TUAB_TRAIN = }\")\n",
    "print(f\"{TUAB_EVAL = }\")\n",
    "print()\n",
    "\n",
    "print(f\"{DEPR_ANON_DIRECTORY = }\")\n",
    "print()\n",
    "\n",
    "print(f\"{INHOUSE_DIRECTORY = }\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd98e527-c441-420e-8865-eabab1d7df0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "from IPython.display import display, clear_output\n",
    "from tqdm.auto import tqdm as tqdm_auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f988a65b-4db3-4b7e-b92f-88c81125af62",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(SRC_FOLDER)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport utils\n",
    "\n",
    "from utils import SEED\n",
    "from utils.data_preprocessing import preprocessDepressionAnonymizedData, save_preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec9b57e-452e-4278-b09c-87e4ac3c6598",
   "metadata": {
    "id": "1s3ANvBMpFLR",
    "papermill": {
     "duration": 0.236639,
     "end_time": "2023-12-09T21:36:41.387444",
     "exception": false,
     "start_time": "2023-12-09T21:36:41.150805",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ced1faa6",
   "metadata": {
    "papermill": {
     "duration": 0.291434,
     "end_time": "2023-12-09T21:36:44.349003",
     "exception": false,
     "start_time": "2023-12-09T21:36:44.057569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tree(data_dir, n_files=2):\n",
    "    for dir, _, filenames in os.walk(data_dir):\n",
    "        print(dir)\n",
    "        for filename in filenames[:n_files]:\n",
    "            print(os.path.join(dir, filename))\n",
    "\n",
    "# tree(DEPR_ANON_DIRECTORY)\n",
    "\n",
    "# tree(INHOUSE_DIRECTORY)\n",
    "\n",
    "# tree(TUAB_DIRECTORY)\n",
    "# raw = mne.io.read_raw_edf(TUAB_TRAIN + \"/aaaaaaff_s002_t000.edf\", preload=False)\n",
    "# print(raw.ch_names, \"\\nDuration:\", raw.times[-1]/60, \"m\")\n",
    "# display(raw.info)\n",
    "# raw.plot(n_channels=21, duration=100, scalings=100e-6, start=20);#.set_size_inches(5, 5, forward=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b8a79b-6c6a-4367-aa44-4b44534c635a",
   "metadata": {
    "id": "sIavQ0ClpFLS",
    "papermill": {
     "duration": 0.23423,
     "end_time": "2023-12-09T21:36:45.770298",
     "exception": false,
     "start_time": "2023-12-09T21:36:45.536068",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data preprocessing and saving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a4d845-627d-4bc3-b194-08ee5c775833",
   "metadata": {
    "papermill": {
     "duration": 0.237226,
     "end_time": "2023-12-09T21:36:46.242824",
     "exception": false,
     "start_time": "2023-12-09T21:36:46.005598",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### TUAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5596f1a4",
   "metadata": {
    "id": "CbpWbWddpFLT",
    "outputId": "5330dcd7-ba7c-47b2-b4d8-941d4b19a133",
    "papermill": {
     "duration": 0.242559,
     "end_time": "2023-12-09T21:36:46.724429",
     "exception": false,
     "start_time": "2023-12-09T21:36:46.481870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def remove_files(dir, tags):\n",
    "#     for directory, _, file_names in os.walk(TUAB_DIRECTORY):\n",
    "#         print(directory)\n",
    "#         for file_name in file_names:\n",
    "#             if \".csv_chunks_fz_cz_pz_3x124.npy\" in file_name or \".csv_targets.npy\" in file_name:\n",
    "#                 os.remove(os.path.join(directory, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3146c343",
   "metadata": {
    "id": "Vm0PAEBYpFLT",
    "papermill": {
     "duration": 0.254858,
     "end_time": "2023-12-09T21:36:47.217880",
     "exception": false,
     "start_time": "2023-12-09T21:36:46.963022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def readCsv(file_name):\n",
    "#     file_obj = open(file_name, \"r\")\n",
    "#     age = int(file_obj.readline()[8:-1])\n",
    "#     data = file_obj.readlines()\n",
    "#     file_obj.close()\n",
    "#     data[0] = data[0].replace(\"# \", \"\")\n",
    "#     processed_file_name = str(age) + \".csv\"\n",
    "#     processed_file_obj = open(processed_file_name, \"w\")\n",
    "#     processed_file_obj.writelines(data)\n",
    "#     df = pd.read_csv(processed_file_name)\n",
    "#     processed_file_obj.close()\n",
    "#     os.remove(processed_file_name)\n",
    "#     return df, age\n",
    "\n",
    "# def df2edf(df, sfreq=SAMPLING_FREQUENCY):\n",
    "#     ch_names = df.columns.to_list()\n",
    "#     ch_types = ['eeg'] * len(ch_names)\n",
    "#     info = mne.create_info(ch_names, ch_types=ch_types, sfreq=sfreq)\n",
    "#     return mne.io.RawArray(df[ch_names].to_numpy(copy=True).T / 1e6, info, verbose=False) #data in microvolts\n",
    "\n",
    "# def selectChunks(df, ch_names, chunk_duration=124, n_chunks_max=None):\n",
    "#     chunks = []\n",
    "#     start_idx = 0\n",
    "#     end_idx = start_idx + chunk_duration\n",
    "\n",
    "#     while end_idx <= df.shape[0]:\n",
    "#         if n_chunks_max != None and len(chunks) >= n_chunks_max:\n",
    "#             break\n",
    "\n",
    "#         chunk = df.iloc[start_idx:end_idx]\n",
    "#         if len(chunk) != chunk_duration:\n",
    "#             print(f\"WARNING: chunk shape = {chunk.shape}\")\n",
    "#             start_idx = end_idx\n",
    "#             end_idx += chunk_duration\n",
    "#             continue\n",
    "\n",
    "#         #std = 5 threshold\n",
    "#         if chunk[ch_names].to_numpy().std() >= 5:\n",
    "#             start_idx = end_idx\n",
    "#             end_idx += chunk_duration\n",
    "#             continue\n",
    "\n",
    "#         # std t, t+1 threshold\n",
    "#         # drop = False\n",
    "#         # for col in chunk[ch_names]:\n",
    "#         #     timeseries = chunk[col].to_numpy()\n",
    "#         #     for i in range(len(timeseries) - 1):\n",
    "#         #         if np.std([timeseries[i], timeseries[i + 1]]) >= 3:\n",
    "#         #             drop = True\n",
    "#         #             break\n",
    "#         #     if drop:\n",
    "#         #         break\n",
    "#         # if drop:\n",
    "#         #     continue\n",
    "\n",
    "#         chunks.append(chunk)\n",
    "#         start_idx = end_idx\n",
    "#         end_idx += chunk_duration\n",
    "\n",
    "#     return chunks\n",
    "\n",
    "# def getAge(file_name):\n",
    "#     f = open(file_name, \"r\", encoding=\"utf-8\")\n",
    "#     try:\n",
    "#         buffer = \"1234\"\n",
    "#         while buffer != \"Age:\":\n",
    "#             buffer = buffer[1:] + f.read(1)\n",
    "\n",
    "#         age = f.read(2)\n",
    "#         f.close()\n",
    "#         return int(age)\n",
    "\n",
    "#     except (UnicodeDecodeError, KeyboardInterrupt, ValueError) as error:\n",
    "#         f.close()\n",
    "#         return \"error\"\n",
    "\n",
    "# def preprocessRecord(\n",
    "#         file_name,\n",
    "#         verbose=False,\n",
    "#         channels_to_drop=['EEG ROC-REF', 'EEG LOC-REF', 'EEG EKG1-REF', 'PHOTIC-REF', 'IBI', 'BURSTS', 'SUPPR'],\n",
    "#         sfreq=125,\n",
    "#         l_freq=L_FREQ,\n",
    "#         h_freq=H_FREQ,\n",
    "#         ampl_thresh=AMPLITUDE_THRESHOLD\n",
    "#     ):\n",
    "#     #average reference, filtration\n",
    "#     raw = mne.io.read_raw_edf(file_name, preload=False, verbose=verbose) #data in microvolts\n",
    "#     raw = raw.drop_channels(channels_to_drop, on_missing=\"warn\")\n",
    "#     raw = raw.resample(sfreq, npad='auto')\n",
    "#     raw, _ = mne.set_eeg_reference(raw, ref_channels='average', verbose=verbose) #average reference\n",
    "#     raw.filter(l_freq=l_freq, h_freq=h_freq, method='iir', verbose=verbose) #filtration\n",
    "#     df = raw.to_data_frame() #in microvolts\n",
    "#     ch_names = raw.ch_names\n",
    "\n",
    "#     #clipping\n",
    "#     df.loc[:, ch_names].clip(-ampl_thresh, ampl_thresh, inplace=True)\n",
    "\n",
    "#     #normalization\n",
    "#     df.loc[:, ch_names] = (df[ch_names] - df[ch_names].mean())/df[ch_names].std()\n",
    "\n",
    "#     return df, ch_names #time is also among columns\n",
    "\n",
    "# def processDirectoryData(\n",
    "#         directory,\n",
    "#         picked_channels,\n",
    "#         n_files=None,\n",
    "#         file_type = \"edf\",\n",
    "#         chunks_file_suffix = \"_chunks_fz_cz_pz_3x124\",\n",
    "#         targets_file_suffix = \"_targets\",\n",
    "#         is_save = True,\n",
    "#         is_return = True,\n",
    "#         force_recompute = False,\n",
    "#         **kwargs):\n",
    "#     n_files_read = 0\n",
    "#     n_files_passed = 0\n",
    "\n",
    "#     if n_files is not None:\n",
    "#         file_names = os.listdir(directory)[:n_files]\n",
    "#     else:\n",
    "#         file_names = os.listdir(directory)\n",
    "\n",
    "#     if is_return:\n",
    "#         bunch_of_chunks_list = []\n",
    "#         bunch_of_targets_list = []\n",
    "\n",
    "#     for file_name in tqdm_auto(file_names):\n",
    "#         if fileExtension(file_name) == file_type and \\\n",
    "#             (force_recompute or \\\n",
    "#              not (os.path.exists(directory + fileName(file_name) + chunks_file_suffix + \".npy\") and \\\n",
    "#                   os.path.exists(directory + fileName(file_name) + targets_file_suffix + \".npy\")\\\n",
    "#                  )\\\n",
    "#             ):\n",
    "#             chunks_list = []\n",
    "#             targets_list = []\n",
    "\n",
    "#             age = getAge(directory + file_name)\n",
    "#             if age == \"error\":\n",
    "#                 n_files_passed += 1\n",
    "#                 print(f\"File {file_name} was passed, passed files: {n_files_passed}, read files: {n_files_read}\")\n",
    "#                 continue\n",
    "#             n_files_read += 1\n",
    "\n",
    "#             df, ch_names = preprocessRecord(directory + file_name)\n",
    "#             chunks_from_record = selectChunks(df, ch_names, **kwargs)\n",
    "#             for chunk in chunks_from_record:\n",
    "#                 image = chunk[picked_channels].to_numpy().T\n",
    "#                 chunks_list.append(image)\n",
    "#                 targets_list.append(age)\n",
    "\n",
    "#             chunks = np.array(chunks_list)\n",
    "#             targets = np.array(targets_list)\n",
    "\n",
    "#             if is_return:\n",
    "#                 bunch_of_chunks_list.append(chunks)\n",
    "#                 bunch_of_targets_list.append(targets)\n",
    "\n",
    "#             if is_save:\n",
    "#                 np.save(directory + fileName(file_name) + chunks_file_suffix, chunks)\n",
    "#                 np.save(directory + fileName(file_name) + targets_file_suffix, targets)\n",
    "\n",
    "#     print(f\"Read files: {n_files_read}, passed files: {n_files_passed}\")\n",
    "#     if is_return:\n",
    "#         return np.concatenate(bunch_of_chunks_list), np.concatenate(bunch_of_targets_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8e4cd7",
   "metadata": {
    "id": "NVIjlVM_pFLU",
    "outputId": "fc968780-dbc0-4182-cf59-0a320d8868ff",
    "papermill": {
     "duration": 0.246098,
     "end_time": "2023-12-09T21:36:47.702682",
     "exception": false,
     "start_time": "2023-12-09T21:36:47.456584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# picked_channels = ['EEG FZ-REF', 'EEG CZ-REF', 'EEG PZ-REF']\n",
    "# chunks_file_name = \"chunks_fz_cz_pz_3x124\"\n",
    "# targets_file_name = \"targets\"\n",
    "# for directory in [TUAB_TRAIN, TUAB_EVAL]:\n",
    "#     chunks, targets = processDirectoryData(\n",
    "#         directory,\n",
    "#         picked_channels,\n",
    "#         n_chunks_max=60,\n",
    "#         force_recompute=True,\n",
    "#         chunks_file_suffix=\"_\"+chunks_file_name,\n",
    "#         targets_file_suffix=\"_\"+targets_file_name\n",
    "#     )\n",
    "#     np.save(directory + chunks_file_name, chunks)\n",
    "#     np.save(directory + targets_file_name, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeaca69",
   "metadata": {
    "id": "kNuaGvB5pFLU",
    "outputId": "cec94a2b-aaf4-49d1-ee12-8af4462c20b7",
    "papermill": {
     "duration": 0.249724,
     "end_time": "2023-12-09T21:36:48.188399",
     "exception": false,
     "start_time": "2023-12-09T21:36:47.938675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#concatenate .npy files together\n",
    "# for directory in [TUAB_TRAIN, TUAB_EVAL]:\n",
    "#     print(directory)\n",
    "#     chunks_list = []\n",
    "#     targets_list = []\n",
    "#     for file_name in [directory + chunks_file_name + \".npy\", directory + targets_file_name + \".npy\"]: #delete only files with all chunks\n",
    "#         if os.path.exists(file_name):\n",
    "#             os.remove(file_name)\n",
    "\n",
    "#     for i, file_name in enumerate(tqdm_auto(os.listdir(directory))): #iterate through files from every csv file\n",
    "#         if chunks_file_name in file_name or targets_file_name in file_name:\n",
    "#             data = np.load(directory + file_name)\n",
    "#             if targets_file_name in file_name:\n",
    "#                 targets_list.append(data)\n",
    "#             else:\n",
    "#                 chunks_list.append(data)\n",
    "\n",
    "#     chunks = np.array(chunks_list).reshape(-1, 1, 3, 124)\n",
    "#     targets = np.array(targets_list).reshape(-1)\n",
    "#     print(\"Chunks:\", chunks.shape, \"targets:\", targets.shape)\n",
    "#     prefix = \"train_\" if directory == TUAB_TRAIN else \"eval_\"\n",
    "#     np.save(directory + \"../\" + prefix + chunks_file_name, chunks)\n",
    "#     np.save(directory + \"../\" + prefix + targets_file_name, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f3af9c-c379-4c8c-8ed8-e4f054fff9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# chunks_train = np.load(TUAB_TRAIN + \"chunks_fz_cz_pz_3x124.npy\").reshape(-1, 1, 3, 124)\n",
    "# targets_train = np.load(TUAB_TRAIN + \"targets.npy\")\n",
    "# chunks_val_test = np.load(TUAB_EVAL + \"chunks_fz_cz_pz_3x124.npy\").reshape(-1, 1, 3, 124)\n",
    "# targets_val_test = np.load(TUAB_EVAL + \"targets.npy\")\n",
    "# chunks_val, chunks_test, targets_val, targets_test = train_test_split(chunks_val_test, targets_val_test, test_size=0.5, random_state=SEED, shuffle=False)\n",
    "# print(chunks_train.shape, targets_train.shape, chunks_val.shape, targets_val.shape, chunks_test.shape, targets_test.shape)\n",
    "\n",
    "# save_preprocessed_data(\n",
    "#     {\n",
    "#         \"chunks_train\": chunks_train,\n",
    "#         \"targets_train\": targets_train,\n",
    "#         \"chunks_val\": chunks_val,\n",
    "#         \"targets_val\": targets_val,\n",
    "#         \"chunks_test\": chunks_test,\n",
    "#         \"targets_test\": targets_test\n",
    "#     },\n",
    "#     os.path.join(TUAB_DIRECTORY, \"dataset_fz_cz_pz_3x124.pkl\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3e98ca-cb58-4789-93b2-2ede73c27016",
   "metadata": {
    "papermill": {
     "duration": 0.233809,
     "end_time": "2023-12-09T21:36:48.658001",
     "exception": false,
     "start_time": "2023-12-09T21:36:48.424192",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Depression anonymized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a636e8c",
   "metadata": {
    "papermill": {
     "duration": 0.243968,
     "end_time": "2023-12-09T21:36:49.644411",
     "exception": false,
     "start_time": "2023-12-09T21:36:49.400443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# epoch_folders = [\"epoch_1/\", \"epoch_3/\"]\n",
    "# picked_channels = [\"fz\", \"cz\", \"pz\"]\n",
    "# target_freq = 128\n",
    "# n_samples_per_chunk = target_freq*5\n",
    "# chunks_list = preprocessDepressionAnonymizedData(\n",
    "#     DEPR_ANON_DIRECTORY, \n",
    "#     epoch_folders, \n",
    "#     picked_channels,\n",
    "#     target_freq = target_freq,\n",
    "#     n_samples_per_chunk = n_samples_per_chunk\n",
    "# )\n",
    "# save_preprocessed_data(chunks_list, DEPR_ANON_DIRECTORY + f\"dataset_{target_freq}_{n_samples_per_chunk/target_freq:.1f}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd5c6b4-79b1-4a73-a0a4-9dca6e3c82d1",
   "metadata": {
    "papermill": {
     "duration": 0.234752,
     "end_time": "2023-12-09T21:36:50.595065",
     "exception": false,
     "start_time": "2023-12-09T21:36:50.360313",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Inhouse dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632fc8cf-fd08-4bd4-a8f2-e84821bb31b9",
   "metadata": {
    "papermill": {
     "duration": 0.249712,
     "end_time": "2023-12-09T21:36:51.079292",
     "exception": false,
     "start_time": "2023-12-09T21:36:50.829580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def pickChunks(df, ch_names, n_samples_per_chunk=124):\n",
    "#     chunks = []\n",
    "#     start_idx = 0\n",
    "#     chunk_duration = n_samples_per_chunk\n",
    "#     end_idx = start_idx + chunk_duration\n",
    "\n",
    "#     while end_idx <= df.shape[0]:\n",
    "#         chunk = df.iloc[start_idx:end_idx]\n",
    "#         if len(chunk) != chunk_duration:\n",
    "#             print(f\"WARNING: chunk shape = {chunk.shape}\")\n",
    "#             start_idx = end_idx\n",
    "#             end_idx += chunk_duration\n",
    "#             continue\n",
    "\n",
    "#         #std = 5 threshold\n",
    "#         if chunk[ch_names].to_numpy().std() >= 5:\n",
    "#             start_idx = end_idx\n",
    "#             end_idx += chunk_duration\n",
    "#             continue\n",
    "\n",
    "#         # std t, t+1 threshold\n",
    "#         # drop = False\n",
    "#         # for col in chunk[ch_names]:\n",
    "#         #     timeseries = chunk[col].to_numpy()\n",
    "#         #     for i in range(len(timeseries) - 1):\n",
    "#         #         if np.std([timeseries[i], timeseries[i + 1]]) >= 3:\n",
    "#         #             drop = True\n",
    "#         #             break\n",
    "#         #     if drop:\n",
    "#         #         break\n",
    "#         # if drop:\n",
    "#         #     continue\n",
    "\n",
    "#         chunks.append(chunk)\n",
    "#         start_idx = end_idx\n",
    "#         end_idx += chunk_duration\n",
    "\n",
    "#     return chunks\n",
    "\n",
    "\n",
    "# def processPatientData(df, sfreq=125, sfreq_source=500):\n",
    "#     #average reference, filtration\n",
    "#     ch_names = df.columns.to_list()[1:] #delete 'time' #['Fp1', 'Fp2', 'F3', 'F4', 'Fz', 'C3', 'C4', 'Cz', 'P3', 'P4', 'Pz', 'O1', 'O2', 'F7', 'F8']\n",
    "#     ch_types = ['eeg'] * len(ch_names)\n",
    "#     info = mne.create_info(ch_names, ch_types=ch_types, sfreq=sfreq_source)\n",
    "#     raw = mne.io.RawArray(df[ch_names].to_numpy(copy=True).T / 1e6, info, verbose=False) #data in microvolts\n",
    "#     raw, _ = mne.set_eeg_reference(raw, ref_channels='average', verbose=False) #average reference\n",
    "#     raw.filter(l_freq=L_FREQ, h_freq=H_FREQ, method='iir', verbose=False) #filtration\n",
    "#     raw = raw.resample(sfreq, npad='auto')\n",
    "#     df = raw.to_data_frame() #in microvolts\n",
    "\n",
    "#     #clipping\n",
    "#     df.loc[:, ch_names].clip(-AMPLITUDE_THRESHOLD, AMPLITUDE_THRESHOLD, inplace=True)\n",
    "\n",
    "#     #normalization\n",
    "#     df.loc[:, ch_names] = (df[ch_names] - df[ch_names].mean())/df[ch_names].std()\n",
    "\n",
    "#     return pickChunks(df, ch_names, 124)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d275623-a0c5-4ed2-9444-dae0883581f5",
   "metadata": {
    "papermill": {
     "duration": 0.249019,
     "end_time": "2023-12-09T21:36:51.564406",
     "exception": false,
     "start_time": "2023-12-09T21:36:51.315387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_folders = [\"MDD\", \"Health\"]\n",
    "# picked_channels = [\"Fz\", \"Cz\", \"Pz\"]\n",
    "# chunks_list = []\n",
    "\n",
    "# data_epoch_descr = pd.read_csv(os.path.join(INHOUSE_DIRECTORY, \"path_file.csv\")).drop([\"Unnamed: 0\"], axis=\"columns\")\n",
    "# data_epoch_descr[\"fn\"] = data_epoch_descr[\"fn\"].map(lambda x: x.split(\"/\")[1])\n",
    "# data_epoch_list = readDataExt_mul([os.path.join(INHOUSE_DIRECTORY, x) for x in data_folders], is_list=True)\n",
    "# data_epoch_list_ = []\n",
    "# for df_list in data_epoch_list: data_epoch_list_.extend(df_list)\n",
    "# data_epoch_list = data_epoch_list_\n",
    "\n",
    "# for df in tqdm(data_epoch_list):\n",
    "#     chunks_from_patient = processPatientData(df.drop(['file_name', \"Unnamed: 0\"], axis=1))\n",
    "#     file_mask = data_epoch_descr[\"fn\"] == df['file_name'].iloc[0]\n",
    "#     target = data_epoch_descr[file_mask].iloc[0]['target']\n",
    "\n",
    "#     for chunk in chunks_from_patient:\n",
    "#         image = chunk[picked_channels].to_numpy().T\n",
    "#         chunks_list.append({\n",
    "#             \"chunk\": image,\n",
    "#             \"target\": target,\n",
    "#             \"patient\": df['file_name'].iloc[0]\n",
    "#         })\n",
    "\n",
    "# print(\"\\nChunks shape:\", chunks_list[0][\"chunk\"].shape, \"length:\", len(chunks_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35b8ee6-3e3b-42f2-9e0e-af6f1213582d",
   "metadata": {
    "papermill": {
     "duration": 0.245826,
     "end_time": "2023-12-09T21:36:52.046030",
     "exception": false,
     "start_time": "2023-12-09T21:36:51.800204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with open(os.path.join(INHOUSE_DIRECTORY + \"dataset.pkl\"), \"wb\") as f:\n",
    "#     pickle.dump(chunks_list, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
