{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde1c89e-a7fb-412b-ae7b-58f9e4232d45",
   "metadata": {},
   "source": [
    "# Libraries installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f16a32f-5c26-4217-b41b-46495e5c0aaa",
   "metadata": {
    "editable": true,
    "id": "qADYGk7ZpvHb",
    "outputId": "b376d164-1a61-4cc3-f008-17f63ae82bcd",
    "papermill": {
     "duration": 0.242633,
     "end_time": "2023-12-09T21:36:16.886705",
     "exception": false,
     "start_time": "2023-12-09T21:36:16.644072",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip uninstall torch torchvision torchaudio -y\n",
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\n",
    "# !pip install mne -q\n",
    "# !pip install wandb -q\n",
    "# !pip install tensorboard -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1ddf29-61ce-4110-adbc-85bbcc5bb5a0",
   "metadata": {
    "id": "u9hcW97IpFLP",
    "papermill": {
     "duration": 0.23434,
     "end_time": "2023-12-09T21:36:17.845916",
     "exception": false,
     "start_time": "2023-12-09T21:36:17.611576",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Constants and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc489da4-94c3-47da-9f16-dfc99da8681b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux\n",
      "\n",
      "PROJECT_FOLDER = ''\n",
      "SRC_FOLDER = ''\n",
      "OUTPUT_FOLDER = ''\n",
      "\n",
      "TUAB_DIRECTORY = 'Data/TUAB/'\n",
      "TUAB_TRAIN = 'Data/TUAB/train/normal/01_tcp_ar/'\n",
      "TUAB_EVAL = 'Data/TUAB/eval/normal/01_tcp_ar/'\n",
      "\n",
      "DEPR_ANON_DIRECTORY = 'Data/depression_anonymized/'\n",
      "\n",
      "INHOUSE_DIRECTORY = 'Data/inhouse_dataset/EEG_baseline_with_markers_cleaned/preprocessed_data/EEG_baseline/'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "In this section one defines environment variables. \n",
    "Because I used this notebook on number of machines, I implemented class especially for this. \n",
    "You may not needed in one and use just simple definitions.\n",
    "'''\n",
    "\n",
    "from system_variables import SystemVariables\n",
    "\n",
    "# choose system according your current machine\n",
    "# SYSTEM_NAME = \"Windows\"\n",
    "# SYSTEM_NAME = \"Colab\"\n",
    "# SYSTEM_NAME = \"Kaggle\"\n",
    "SYSTEM_NAME = \"Linux\"\n",
    "\n",
    "sv = SystemVariables(SYSTEM_NAME)\n",
    "PROJECT_FOLDER = sv.get_project_folder()\n",
    "SRC_FOLDER = sv.get_src_folder()\n",
    "OUTPUT_FOLDER = sv.get_output_folder()\n",
    "TUAB_DIRECTORY, TUAB_TRAIN, TUAB_EVAL = sv.get_TUAB_folders()\n",
    "DEPR_ANON_DIRECTORY = sv.get_depr_anon_folder()\n",
    "INHOUSE_DIRECTORY = sv.get_inhouse_folder()\n",
    "\n",
    "print(SYSTEM_NAME)\n",
    "print()\n",
    "\n",
    "print(f\"{PROJECT_FOLDER = }\")\n",
    "print(f\"{SRC_FOLDER = }\")\n",
    "print(f\"{OUTPUT_FOLDER = }\")\n",
    "print()\n",
    "\n",
    "print(f\"{TUAB_DIRECTORY = }\")\n",
    "print(f\"{TUAB_TRAIN = }\")\n",
    "print(f\"{TUAB_EVAL = }\")\n",
    "print()\n",
    "\n",
    "print(f\"{DEPR_ANON_DIRECTORY = }\")\n",
    "print()\n",
    "\n",
    "print(f\"{INHOUSE_DIRECTORY = }\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f042a9-5514-49bf-bff7-83a945b55f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "from IPython.display import display, clear_output\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "rc('animation', html='jshtml')\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.metrics import accuracy_score, f1_score, average_precision_score, roc_auc_score\n",
    "\n",
    "from tqdm.auto import tqdm as tqdm_auto\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "%load_ext tensorboard\n",
    "\n",
    "import wandb\n",
    "!wandb login 1b8e8dc9dcf1a34397a04197c4826d3fe7441dae\n",
    "\n",
    "import mne\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e037a53a-6b68-47e0-9626-fc39f738a70c",
   "metadata": {
    "id": "xhokb8oipFLR",
    "outputId": "b86fecdb-1101-40ee-df27-23de85372207",
    "papermill": {
     "duration": 22.337897,
     "end_time": "2023-12-09T21:36:40.911773",
     "exception": false,
     "start_time": "2023-12-09T21:36:18.573876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append(SRC_FOLDER)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport utils\n",
    "\n",
    "from utils import SEED\n",
    "from utils.common import objectName, seed_all, printLog, upd, Config\n",
    "from utils.models_evaluation import evaluateClassifier, evaluateRegressor, evaluateClassifier_inner_outer_cv\n",
    "from utils.data_reading import DataReader\n",
    "from utils.plotting import dataset_hists, plotData, dict_to_df, printDatasetMeta, printDataloaderMeta, plotSamplesFromDataset\n",
    "from utils.dataset import InMemoryDataset\n",
    "from utils.logger import Logger\n",
    "from utils.parser import parse_ml_config\n",
    "\n",
    "from models import get_model, load_weights_from_wandb\n",
    "from models.modules import encoder_conv, decoder_conv, encoder_conv4, decoder_conv4\n",
    "from models.VAE import VAE, BetaVAE_H, BetaVAE_B\n",
    "from models.AE import AE, AE_framework\n",
    "from models.UNet import UNet\n",
    "\n",
    "from training import train_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c60bd8b-042b-4f80-b288-fe9370333a6e",
   "metadata": {},
   "source": [
    "# Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e185f254-e372-4281-a932-360a1e65e2f9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(config, verbose=0):\n",
    "    try:\n",
    "        if config[\"log_path\"] is not None: logfile = open(os.path.join(config[\"log_path\"], config[\"model\"][\"model_description\"].replace(\" \", \"_\").replace(\"/\", \".\")), \"a\")\n",
    "        else: logfile = None\n",
    "        printLog('#################### ' + config[\"model\"][\"model_description\"] + ' ####################', logfile=logfile)\n",
    "        printLog(json.dumps(config, indent=4), logfile=logfile)\n",
    "        \n",
    "        #Data reading\n",
    "        if verbose - 1 > 0: printLog(\"Data reading\", logfile=logfile)\n",
    "        if config[\"dataset\"][\"train\"] == config[\"dataset\"][\"val\"] == config[\"dataset\"][\"test\"]:\n",
    "            reader = DataReader(\n",
    "                config[\"dataset\"][\"train\"][\"file\"], \n",
    "                dataset_type=config[\"dataset\"][\"train\"][\"name\"],\n",
    "                verbose=(verbose-1)\n",
    "            )\n",
    "            train_set, val_set, test_set = reader.split(\n",
    "                train_size=config[\"dataset\"][\"train\"][\"size\"], val_size=config[\"dataset\"][\"val_size\"], test_size=config[\"dataset\"][\"test_size\"]\n",
    "            )    \n",
    "            \n",
    "        elif config[\"dataset\"][\"val\"] == config[\"dataset\"][\"test\"]:\n",
    "            train_reader = DataReader(\n",
    "                config[\"dataset\"][\"train\"][\"file\"],\n",
    "                dataset_type=config[\"dataset\"][\"train\"][\"name\"],\n",
    "                verbose=(verbose-1),\n",
    "            )\n",
    "            train_set, _, _ = train_reader.split(\n",
    "                train_size=config[\"dataset\"][\"train\"][\"size\"], val_size=0, test_size=0\n",
    "            )\n",
    "\n",
    "            val_test_reader = DataReader(\n",
    "                config[\"dataset\"][\"test\"][\"file\"],\n",
    "                dataset_type=config[\"dataset\"][\"test\"][\"name\"],\n",
    "                verbose=(verbose-1),\n",
    "            )\n",
    "            _, val_set, test_set = val_test_reader.split(\n",
    "                train_size=0, val_size=config[\"dataset\"][\"val\"][\"size\"], test_size=config[\"dataset\"][\"test\"][\"size\"]\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            raise NotImplementedError(\"Unsupported datasets configuration\")\n",
    "\n",
    "        chunks_train, chunks_val, chunks_test = train_set[\"chunk\"], val_set[\"chunk\"], test_set[\"chunk\"]\n",
    "        targets_train, targets_val, targets_test = train_set[\"target\"], val_set[\"target\"], test_set[\"target\"]\n",
    "        \n",
    "        upd(config[\"dataset\"], {\n",
    "            \"samples_shape\": chunks_train[0].shape,\n",
    "            \"train\": {\"n_samples\": len(chunks_train)},\n",
    "            \"val\": {\"n_samples\": len(chunks_val)},\n",
    "            \"test\": {\"n_samples\": len(chunks_test)},\n",
    "        })\n",
    "    \n",
    "        t_max=None\n",
    "        train_dataset = InMemoryDataset(chunks_train, is_squeeze=False, is_unsqueeze=False, t_max=t_max)\n",
    "        val_dataset = InMemoryDataset(chunks_val, is_squeeze=False, is_unsqueeze=False, t_max=t_max)\n",
    "        test_dataset = InMemoryDataset(chunks_test, is_squeeze=False, is_unsqueeze=False, t_max=t_max)\n",
    "    \n",
    "        if verbose - 2 > 0: \n",
    "            printDatasetMeta(train_dataset, val_dataset, test_dataset)\n",
    "            plotSamplesFromDataset(train_dataset)\n",
    "    \n",
    "        #Dataloader\n",
    "        train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=config[\"dataset\"]['batch_size'], num_workers=config[\"dataset\"]['num_workers'])\n",
    "        val_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=config[\"dataset\"]['batch_size'], num_workers=config[\"dataset\"]['num_workers'])\n",
    "        test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=config[\"dataset\"]['batch_size'], num_workers=config[\"dataset\"]['num_workers'])\n",
    "    \n",
    "        if verbose - 2 > 0: printDataloaderMeta(train_dataloader, val_dataloader, test_dataloader)\n",
    "    \n",
    "        #Model\n",
    "        config[\"model\"].update({\n",
    "            \"input_dim\" : train_dataset[0].shape,\n",
    "        })\n",
    "        model, config[\"model\"] = get_model(config[\"model\"])\n",
    "        model = model.to(device)\n",
    "        if verbose - 1 > 0: printLog('model ' + config[\"model\"]['model_description'] + ' is created', logfile=logfile)\n",
    "    \n",
    "        #Download weights\n",
    "        if \"artifact\" in config[\"model\"] and \"file\" in config[\"model\"]:\n",
    "            model = load_weights_from_wandb(model, config[\"model\"][\"artifact\"], config[\"model\"][\"file\"], verbose=verbose)\n",
    "    \n",
    "        # TESTS\n",
    "        model.eval()\n",
    "        test_data_point = train_dataset[0][None].to(device)\n",
    "        inference_result = model(test_data_point)\n",
    "        reconstruct_result = model.reconstruct(test_data_point)\n",
    "        encode_result = model.encode(test_data_point)\n",
    "        if verbose - 1 > 0: \n",
    "            printLog(f\"Test data point shape: {test_data_point.shape}\", logfile=logfile)\n",
    "            printLog(f\"Test inference: {len(inference_result)}\", logfile=logfile)\n",
    "            printLog(f\"Test reconstruct: {reconstruct_result.shape}\", logfile=logfile)\n",
    "            printLog(f\"Test encode: {encode_result.shape}\", logfile=logfile)\n",
    "    \n",
    "        #optimizer and scheduler\n",
    "        optimizer = getattr(torch.optim, config[\"optimizer\"][\"optimizer\"])(model.parameters(), **config[\"optimizer\"][\"kwargs\"])\n",
    "        if verbose - 1 > 0: printLog(f'Optimizer {type(optimizer).__name__} is instantiated', logfile=logfile)\n",
    "    \n",
    "        scheduler = getattr(torch.optim.lr_scheduler, config[\"scheduler\"][\"scheduler\"])(optimizer, **config[\"scheduler\"][\"kwargs\"])\n",
    "        if verbose - 1 > 0: printLog(f'Scheduler {type(scheduler).__name__} is instantiated', logfile=logfile)\n",
    "    \n",
    "        logger = Logger(\n",
    "            log_type=config[\"logger\"][\"log_type\"], \n",
    "            run_name=config[\"model\"][\"model_description\"],\n",
    "            save_path=config[\"save_path\"],\n",
    "            model=model,\n",
    "            model_name=config[\"model\"][\"model\"],        \n",
    "            project_name=config[\"project_name\"],\n",
    "            config=config,\n",
    "            model_description=config[\"model\"][\"model_description\"],\n",
    "        #         log_dir = OUTPUT_FOLDER + \"logs/\"\n",
    "        )\n",
    "    \n",
    "        #parse ml config\n",
    "        #should be just before training because replace names by objects\n",
    "        config[\"ml\"] = parse_ml_config(config[\"ml\"])\n",
    "    \n",
    "        #seed\n",
    "        seed_all(SEED)\n",
    "    \n",
    "        #training\n",
    "        # best_loss = np.inf\n",
    "        best_clf_accuracy = -np.inf\n",
    "        best_model = None\n",
    "        best_epoch = None\n",
    "        final_model = None\n",
    "        \n",
    "        for epoch in tqdm_auto(range(config[\"train\"]['start_epoch'], config[\"train\"]['end_epoch'])):\n",
    "            if verbose > 0: printLog(f\"Epoch {epoch}\", logfile=logfile)\n",
    "            \n",
    "            #######\n",
    "            # train\n",
    "            #######\n",
    "            if verbose > 0: printLog(\"##### Training... #####\", logfile=logfile)\n",
    "            model, results = train_eval(\n",
    "                train_dataloader,\n",
    "                model,\n",
    "                device=device,\n",
    "                mode=\"train\",\n",
    "                optimizer=optimizer,\n",
    "                epoch=epoch,\n",
    "                logger=logger,\n",
    "                loss_coefs=config[\"train\"][\"loss_coefs\"],\n",
    "                loss_reduction=config[\"model\"][\"loss_reduction\"],\n",
    "                is_mask=(config[\"train\"][\"masking\"][\"n_masks\"] != 0 and config[\"train\"][\"masking\"][\"mask_ratio\"] != 0),\n",
    "                mask_ratio=config[\"train\"][\"masking\"][\"mask_ratio\"],\n",
    "                step_max=config[\"train\"][\"step_max\"], \n",
    "            )\n",
    "            if results == {}: break\n",
    "            if verbose > 0: \n",
    "                display(dict_to_df(results))\n",
    "                for k in results: \n",
    "                    if isinstance(results[k], np.ndarray): results[k] = float(results[k].tolist())\n",
    "                print(json.dumps(results, indent=4), file=logfile)\n",
    "    \n",
    "            ############\n",
    "            # validation\n",
    "            ############\n",
    "            if verbose > 0: printLog(\"##### Validation... #####\", logfile=logfile)\n",
    "            model, results = train_eval(\n",
    "                val_dataloader,\n",
    "                model,\n",
    "                device=device,\n",
    "                mode=\"validation\",\n",
    "                test_dataset=val_dataset,\n",
    "                targets_test=targets_val,\n",
    "                check_period=config[\"train\"][\"validation\"][\"check_period\"],\n",
    "                plot_period=config[\"train\"][\"validation\"][\"plot_period\"],\n",
    "                epoch=epoch,\n",
    "                logger=logger,\n",
    "                loss_coefs=config[\"train\"][\"loss_coefs\"],\n",
    "                loss_reduction=config[\"model\"][\"loss_reduction\"],\n",
    "                is_mask=(config[\"train\"][\"masking\"][\"n_masks\"] != 0 and config[\"train\"][\"masking\"][\"mask_ratio\"] != 0),\n",
    "                mask_ratio=config[\"train\"][\"masking\"][\"mask_ratio\"],\n",
    "                step_max=config[\"train\"][\"step_max\"], \n",
    "                **config[\"ml\"],\n",
    "            )\n",
    "            if results == {}: break\n",
    "            if verbose > 0: \n",
    "                display(dict_to_df(results))\n",
    "                for k in results: \n",
    "                    if type(results[k]) == np.ndarray: results[k] = float(results[k].tolist())\n",
    "                print(json.dumps(results, indent=4), file=logfile)\n",
    "    \n",
    "            scheduler.step(results['loss'])\n",
    "            logger.save_model(epoch)\n",
    "            final_model = model\n",
    "    \n",
    "            zero_ml_tag = config[\"ml\"][\"ml_eval_function_tag\"][0]\n",
    "            if results[f'clf.{zero_ml_tag}.test.accuracy.cv'] >= best_clf_accuracy:\n",
    "                best_clf_accuracy = results[f'clf.{zero_ml_tag}.test.accuracy.cv']\n",
    "                best_model = model\n",
    "                best_epoch = best_epoch\n",
    "                if verbose > 0: printLog(f\"New best classifier accuracy = {best_clf_accuracy} on epoch {epoch}\", logfile=logfile)\n",
    "            \n",
    "            # if results['loss'] < best_loss:\n",
    "            #     best_loss = results['loss']\n",
    "            #     best_model = model\n",
    "            #     best_epoch = best_epoch\n",
    "            #     if verbose > 0: printLog(f\"New best loss = {best_loss} on epoch {epoch}\", logfile=logfile)\n",
    "    \n",
    "        logger.save_model(config[\"train\"]['end_epoch'])\n",
    "    \n",
    "        ######\n",
    "        # test\n",
    "        ######\n",
    "        results_all = {}\n",
    "        for model, mode in zip([final_model, best_model], [\"final\", \"test\"]):\n",
    "            if verbose > 0: printLog(f\"##### Testing in {mode} mode... #####\", logfile=logfile)\n",
    "            _, results = train_eval(\n",
    "                test_dataloader,\n",
    "                model,\n",
    "                device=device,\n",
    "                mode=mode,\n",
    "                test_dataset=test_dataset,\n",
    "                targets_test=targets_test,\n",
    "                check_period=1e10,\n",
    "                plot_period=1e10,\n",
    "                epoch=config[\"train\"]['end_epoch'],\n",
    "                logger=logger,\n",
    "                loss_coefs=config[\"train\"][\"loss_coefs\"],\n",
    "                loss_reduction=config[\"model\"][\"loss_reduction\"],\n",
    "                is_mask=(config[\"train\"][\"masking\"][\"n_masks\"] != 0 and config[\"train\"][\"masking\"][\"mask_ratio\"] != 0),\n",
    "                mask_ratio=config[\"train\"][\"masking\"][\"mask_ratio\"],\n",
    "                step_max=config[\"train\"][\"step_max\"], \n",
    "                **config[\"ml\"],\n",
    "            )\n",
    "            results_all[mode] = results\n",
    "            if verbose > 0: \n",
    "                display(dict_to_df(results))\n",
    "                for k in results: \n",
    "                    if type(results[k]) == np.ndarray: results[k] = float(results[k].tolist())\n",
    "                print(json.dumps(results, indent=4), file=logfile)\n",
    "        \n",
    "        logger.update_summary(\"validation.best_epoch\", best_epoch)\n",
    "        logger.finish()\n",
    "\n",
    "        logfile.close()\n",
    "        return results_all\n",
    "        \n",
    "    except Exception as error:\n",
    "        # handle the exception\n",
    "        exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "        traceback.print_exception(exc_type, exc_value, exc_traceback) \n",
    "        if logfile is not None: \n",
    "            traceback.print_exception(exc_type, exc_value, exc_traceback, file=logfile) \n",
    "            logfile.close()\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a880451f-35ee-4bfc-ad48-225b55c5b0b7",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c756beee-8cef-4905-ba5d-d1ca44f9eba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = {\n",
    "    \"loss_coefs\": {\n",
    "        \"ampl\": 1,\n",
    "        \"vel\": 0,\n",
    "        \"acc\": 0,\n",
    "        \"frq\": 0,\n",
    "        \"kl\": 1\n",
    "    },\n",
    "    \"masking\" :{\n",
    "        \"n_masks\" : 0, #0/1\n",
    "        \"mask_ratio\" : 0 #[0, 1]\n",
    "    },\n",
    "    \n",
    "    \"start_epoch\": 0, # including\n",
    "    \"end_epoch\": 1, # excluding,\n",
    "    \"step_max\" : None,\n",
    "\n",
    "    \"validation\": {\n",
    "        \"check_period\": 1e10,\n",
    "        \"plot_period\": None, #1e10\n",
    "    }\n",
    "}\n",
    "\n",
    "logger_config = {\n",
    "    \"log_type\" : \"wandb\", #\"wandb\"/\"tensorboard\"/\"none\"\n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    \"model\": \"AE\",\n",
    "    \"loss_reduction\" : \"mean\",\n",
    "    \"model_description\": \"TUAB+inhouse, AE, 3 ch., 4/8/16/32, 7/7/5/3/3/3/3/1, Sigmoid\",\n",
    "    # \"artifact\" : 'dmitriykornilov_team/EEG_age_prediction/AE:v18',\n",
    "    # \"file\": '50_epoch.pth'\n",
    "}\n",
    "\n",
    "dataset_config = {\n",
    "    \"batch_size\": 256,\n",
    "    \"num_workers\": 0,\n",
    "    \"train\": {\n",
    "        \"name\": \"TUAB\", #inhouse_dataset/depression_anonymized/TUAB\n",
    "        \"file\": TUAB_DIRECTORY + \"dataset_128_1.0.pkl\",\n",
    "        \"size\": None,\n",
    "    },\n",
    "    \"val\": {\n",
    "        \"name\": \"inhouse_dataset\", #inhouse_dataset/depression_anonymized/TUAB\n",
    "        \"file\": INHOUSE_DIRECTORY + \"dataset_128_1.0.pkl\",\n",
    "        \"size\": 0.5,\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"name\": \"inhouse_dataset\", #inhouse_dataset/depression_anonymized/TUAB\n",
    "        \"file\": INHOUSE_DIRECTORY + \"dataset_128_1.0.pkl\",\n",
    "        \"size\": 0.5,\n",
    "    },\n",
    "}\n",
    "\n",
    "optimizer_config = {\n",
    "    \"optimizer\" : \"AdamW\",\n",
    "    \"kwargs\": {\n",
    "        \"lr\": 1e-3\n",
    "    }\n",
    "}\n",
    "\n",
    "scheduler_config = {\n",
    "    \"scheduler\" : \"ReduceLROnPlateau\",\n",
    "    \"kwargs\": {\n",
    "        \"factor\": 0.5,\n",
    "        \"patience\": 3, \n",
    "        \"verbose\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "ml_config = {\n",
    "    \"avg_embeddings_over_time\": True,\n",
    "    \"plot_type\": \"classification\", #\"regression\"/\"classification\"\n",
    "    \"ml_model\": {\n",
    "        \"scaler\" : \"preprocessing.StandardScaler\",\n",
    "        \"clf\": \"svm.SVC\",\n",
    "    },\n",
    "    \"ml_param_grid\" : {\n",
    "        'clf__kernel': ['linear'],\n",
    "        'clf__C': list(np.logspace(-1, 1, 3)),\n",
    "        'clf__probability': [True],\n",
    "        'clf__class_weight': ['balanced'],\n",
    "        'clf__random_state': [SEED]\n",
    "    },\n",
    "    \"ml_eval_function\" : [\n",
    "        \"evaluateClassifier_inner_outer_cv\",\n",
    "        \"evaluateClassifier\", \n",
    "    ],\n",
    "    \"ml_eval_function_tag\" : [\"cv\", \"d\"],\n",
    "    \"ml_eval_function_kwargs\" : [\n",
    "        {\n",
    "            \"verbose\" : 0,\n",
    "            \"SEED\" : SEED,\n",
    "            \"cv_scorer\" : \"accuracy_score\",\n",
    "            \"metrics\" : [(\"average_precision_score\", \"soft\"), (\"roc_auc_score\", \"soft\"), (\"accuracy_score\", \"hard\"), (\"f1_score\", \"hard\")],\n",
    "            \"n_splits_inner\" : 5,\n",
    "            \"n_splits_outer\" : 10,\n",
    "        },\n",
    "        {\n",
    "            \"verbose\" : 0,\n",
    "            \"test_size\" : 0.33,\n",
    "            \"SEED\" : SEED,\n",
    "            \"cv_scorer\" : \"accuracy_score\",\n",
    "            \"metrics\" : [(\"average_precision_score\", \"soft\"), (\"roc_auc_score\", \"soft\"), (\"accuracy_score\", \"hard\"), (\"f1_score\", \"hard\")],\n",
    "            # \"metrics_for_CI\" : [(\"average_precision_score\", \"soft\"), (\"roc_auc_score\", \"soft\"), (\"accuracy_score\", \"hard\"), (\"f1_score\", \"hard\")],\n",
    "            # \"n_bootstraps\" : 1000\n",
    "        },\n",
    "    ],\n",
    "    \"ml_metric_prefix\" : \"clf\",\n",
    "    \n",
    "    # ml_model=Pipeline([('scaler', StandardScaler()), ('reg', Ridge())]),\n",
    "    # ml_param_grid={\n",
    "    #     'reg__alpha': np.logspace(-2, 2, 5),\n",
    "    #     # 'reg__solver': ['sag'],\n",
    "    #     # 'reg__tol': [1e-4]\n",
    "    # },\n",
    "    # ml_eval_function=evaluateRegressor,\n",
    "    # ml_metric_names=[\"mse_test\", \"mse_train\"],\n",
    "    # ml_metric_prefix=\"reg\",\n",
    "}\n",
    "\n",
    "\n",
    "default_config = {\n",
    "    \"project_name\": 'EEG_depression_classification',\n",
    "    \"method\": \"direct restoration\",\n",
    "    \"save_path\" : OUTPUT_FOLDER + 'model_weights/',\n",
    "    \"log_path\" : OUTPUT_FOLDER + \"logs/\",\n",
    "    \n",
    "    \"dataset\": dataset_config,\n",
    "    \"model\": model_config,\n",
    "    \"optimizer\" : optimizer_config,\n",
    "    \"scheduler\": scheduler_config,\n",
    "    \"train\": train_config,\n",
    "    \"ml\": ml_config,\n",
    "    \"logger\": logger_config,\n",
    "}\n",
    "\n",
    "# print(\"Config:\", json.dumps(default_config, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774678f7-4d4e-4eea-b999-e08931e15d47",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b621b73e-25da-40e8-b82d-06e26ed8b3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = Config(default_config)\n",
    "\n",
    "experiments = [\n",
    "    dc.upd({\n",
    "        \"dataset\" : {\n",
    "            \"train\": {\n",
    "                \"name\": \"TUAB\", #inhouse_dataset/depression_anonymized/TUAB\n",
    "                \"file\": TUAB_DIRECTORY + f\"dataset_128_{x}.0.pkl\",\n",
    "                \"size\": None,\n",
    "            },\n",
    "            \"val\": {\n",
    "                \"name\": \"inhouse_dataset\", #inhouse_dataset/depression_anonymized/TUAB\n",
    "                \"file\": INHOUSE_DIRECTORY + f\"dataset_128_{x}.0.pkl\",\n",
    "                \"size\": 0.5,\n",
    "            },\n",
    "            \"test\": {\n",
    "                \"name\": \"inhouse_dataset\", #inhouse_dataset/depression_anonymized/TUAB\n",
    "                \"file\": INHOUSE_DIRECTORY + f\"dataset_128_{x}.0.pkl\",\n",
    "                \"size\": 0.5,\n",
    "            },\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"model_description\": f\"TUAB+inhouse, AE, 3 ch., 4/8/16/32, 7/7/5/3/3/3/3/1, Sigmoid, {x} s\"\n",
    "        }, \n",
    "    })\n",
    "    for x in [1, 2, 4, 5, 10, 15, 30, 60]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7668e523-030a-4a72-ab67-9b9b117be419",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b48e9c-2996-44ba-9cbc-9802fbd69b27",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for config in experiments:\n",
    "    all_results.append(train(config, verbose=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284a524d-339f-46cc-bdc0-4277ecbec044",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
